{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxhqhnUb2nXW"
      },
      "source": [
        "# 1. Importing Libraries ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zAimqCVL2m13"
      },
      "outputs": [],
      "source": [
        "# numpy and scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from scipy.linalg import hadamard\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sdzlyg6cH5BJ"
      },
      "outputs": [],
      "source": [
        "!pip freeze >> requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M32JvtYuSGVv"
      },
      "source": [
        "# 1. Obtaining the 5G LDPC Codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgTJu4mSr_eY"
      },
      "source": [
        "# 2. Copying the LLR ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qer7vLh5sFPW",
        "outputId": "9c796446-874a-4d85-d8de-f7f49999460f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "tensor([0, 0, 1, 1, 2, 3, 3])\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define an example parity-check matrix (H)\n",
        "H = torch.tensor([\n",
        "    [1, 1, 0, 0],  # Check 1\n",
        "    [0, 1, 1, 1],  # Check 2\n",
        "    [1, 0, 0, 1]   # Check 3\n",
        "], dtype=torch.float32)  # Shape: (3, 4)\n",
        "\n",
        "# Transpose H to get H_T\n",
        "H_T = H.T  # Shape: (4, 3)\n",
        "\n",
        "# Example LLR vector (batch_size=2, num_vars=4)\n",
        "LLR = torch.tensor([\n",
        "    [0.0, 0.0, 0.0, 0.0],  # Batch 1\n",
        "    [-0.3, 0.7, -0.9, 0.2]   # Batch 2\n",
        "], dtype=torch.float32)  # Shape: (2, 4)\n",
        "\n",
        "# Step 1: Find indices where H_T == 1\n",
        "indices = (H_T == 1).nonzero(as_tuple=True)  # Returns tuple (row_indices, col_indices)\n",
        "\n",
        "# Count number of 1s aka total number of messages\n",
        "nodes = torch.sum(H == 1).item()\n",
        "print(nodes)\n",
        "print(indices[0]) # row indices\n",
        "# print(indices[1]) # column indices\n",
        "print(\"\\n\")\n",
        "\n",
        "# Step 2: Extract LLR values using row indices (H_T rows correspond to original H columns)\n",
        "# copied_LLR = LLR[:, indices[0]]  # Shape: (batch_size, num_selected_values)\n",
        "# print(input_mapping_LLR[0]) # batch 1\n",
        "# print(input_mapping_LLR[1]) # batch 2\n",
        "\n",
        "# print(\"Copied LLR:\")\n",
        "# print(copied_LLR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGjESNlHvFpX"
      },
      "source": [
        "# CL and VL index mapping ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmURQf4avISY",
        "outputId": "39d82f42-0b1c-4e59-a417-4ad2919af07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H_to_LLR_mapping_T:\n",
            " tensor([[ 0,  2, -1, -1],\n",
            "        [-1,  3,  4,  5],\n",
            "        [ 1, -1, -1,  6]])\n",
            "\n",
            "Check LLR Index Matrix:\n",
            " tensor([[ 2, -1],\n",
            "        [ 6, -1],\n",
            "        [ 0, -1],\n",
            "        [ 4,  5],\n",
            "        [ 3,  5],\n",
            "        [ 3,  4],\n",
            "        [ 1, -1]])\n",
            "\n",
            "Variable LLR Index Matrix:\n",
            " tensor([[ 1],\n",
            "        [ 0],\n",
            "        [ 3],\n",
            "        [ 2],\n",
            "        [-1],\n",
            "        [ 6],\n",
            "        [ 5]])\n"
          ]
        }
      ],
      "source": [
        "def get_LLR_indexes(H_to_LLR_mapping_T):\n",
        "    \"\"\"\n",
        "    Creates a 2D tensor mapping each LLR index to all other indices in the same row.\n",
        "\n",
        "    Args:\n",
        "        H_to_LLR_mapping_T (torch.Tensor): Transposed mapping matrix (num_vars, num_checks)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 2D tensor where each row corresponds to an LLR index,\n",
        "                      and columns contain indices of other LLRs in the same row.\n",
        "                      Uses -1 for padding if needed.\n",
        "    \"\"\"\n",
        "    num_ones = (H_to_LLR_mapping_T >= 0).sum().item()  # Number of valid LLR indices\n",
        "\n",
        "    # Dictionary to store shared indices for each LLR index\n",
        "    check_indices_dict = {i: [] for i in range(num_ones)}\n",
        "    var_indices_dict = {i: [] for i in range(num_ones)}\n",
        "\n",
        "    # Obtain Check Layer LLR Indexes\n",
        "    for row in range(H_to_LLR_mapping_T.shape[0]):\n",
        "        # Get valid indices (ignore -1)\n",
        "        check_indices = H_to_LLR_mapping_T[row][H_to_LLR_mapping_T[row] != -1]\n",
        "\n",
        "        # Map each LLR index to other LLR indices in the same row\n",
        "        for idx in check_indices:\n",
        "            check_indices_dict[idx.item()] = [j.item() for j in check_indices if j != idx]\n",
        "\n",
        "    # Obtain Variable Layer LLR Indexes\n",
        "    for row in range(H_to_LLR_mapping_T.T.shape[0]):\n",
        "        # Get valid indices (ignore -1)\n",
        "        var_indices = H_to_LLR_mapping_T.T[row][H_to_LLR_mapping_T.T[row] != -1]\n",
        "\n",
        "        # Map each LLR index to other LLR indices in the same row\n",
        "        for idx in var_indices:\n",
        "            var_indices_dict[idx.item()] = [j.item() for j in var_indices if j != idx]\n",
        "\n",
        "    # Convert dictionary to a padded 2D tensor\n",
        "    check_max_neighbors = max(len(v) for v in check_indices_dict.values())  # Find max list length\n",
        "    var_max_neighbours = max(len(v) for v in var_indices_dict.values())  # Find max list length\n",
        "\n",
        "    check_indices_matrix = torch.full((num_ones, check_max_neighbors), -1, dtype=torch.long)  # Initialize with -1\n",
        "    var_indices_matrix = torch.full((num_ones, var_max_neighbours), -1, dtype=torch.long)  # Initialize with -1\n",
        "\n",
        "    for i, neighbors in check_indices_dict.items():\n",
        "        check_indices_matrix[i, :len(neighbors)] = torch.tensor(neighbors)\n",
        "\n",
        "    for i, neighbors in var_indices_dict.items():\n",
        "        var_indices_matrix[i, :len(neighbors)] = torch.tensor(neighbors)\n",
        "\n",
        "    return check_indices_matrix , var_indices_matrix\n",
        "\n",
        "# Find indices where H_T == 1\n",
        "row_indices, col_indices = (H_T == 1).nonzero(as_tuple=True)\n",
        "\n",
        "# Create a mapping from (row, col) in H_T to LLR index\n",
        "H_to_LLR_mapping = torch.full_like(H_T, -1, dtype=torch.long)  # Initialize with -1\n",
        "\n",
        "# Assign LLR indices in natural order\n",
        "for i, (row, col) in enumerate(zip(row_indices, col_indices)):\n",
        "    H_to_LLR_mapping[row, col] = i  # Map H index to LLR index\n",
        "\n",
        "# **Transpose the mapping matrix**\n",
        "H_to_LLR_mapping_T = H_to_LLR_mapping.T  # Transpose\n",
        "\n",
        "# Compute the shared LLR tensor\n",
        "check_LLR_matrix , var_LLR_matrix = get_LLR_indexes(H_to_LLR_mapping_T)\n",
        "\n",
        "# Print results\n",
        "print(\"H_to_LLR_mapping_T:\\n\", H_to_LLR_mapping_T)\n",
        "print(\"\\nCheck LLR Index Matrix:\\n\", check_LLR_matrix)\n",
        "print(\"\\nVariable LLR Index Matrix:\\n\", var_LLR_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMcqWnkRCgqm"
      },
      "source": [
        "# Params for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y53YU3jzy_ks",
        "outputId": "51ef636b-8524-4f16-942c-f855b17722ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "check_index_tensor:\n",
            "tensor([[ 2, -1],\n",
            "        [ 6, -1],\n",
            "        [ 0, -1],\n",
            "        [ 4,  5],\n",
            "        [ 3,  5],\n",
            "        [ 3,  4],\n",
            "        [ 1, -1]])\n",
            "\n",
            "var_index_tensor:\n",
            "tensor([[ 1],\n",
            "        [ 0],\n",
            "        [ 3],\n",
            "        [ 2],\n",
            "        [-1],\n",
            "        [ 6],\n",
            "        [ 5]])\n",
            "\n",
            "output_index_tensor:\n",
            "tensor([[0, 0, 1, 1, 2, 3, 3]])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "batch_size = 2\n",
        "num_nodes = 7\n",
        "num_iterations = 2\n",
        "depth_L = 2\n",
        "\n",
        "# mapped LLR indexes for check and variable layer\n",
        "check_index_tensor = check_LLR_matrix\n",
        "var_index_tensor = var_LLR_matrix\n",
        "\n",
        "# check and var tensor\n",
        "print(f\"\\ncheck_index_tensor:\\n{check_index_tensor}\")\n",
        "print(f\"\\nvar_index_tensor:\\n{var_index_tensor}\")\n",
        "\n",
        "# Example Output Mapping Index\n",
        "output_index_tensor = torch.tensor([[0, 0, 1, 1, 2, 3, 3]], dtype=torch.int64)\n",
        "print(f\"\\noutput_index_tensor:\\n{output_index_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIU_h5tQ1lei"
      },
      "source": [
        "# Final Draft 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOr7cHKP1m1H",
        "outputId": "25ea9230-7a85-4d4d-df02-9c90601b2b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]])\n",
            "tensor([[[ 2.4578,  0.0000,  0.0000],\n",
            "         [ 4.9123,  0.0000,  0.0000],\n",
            "         [ 5.5499,  0.0000,  0.0000],\n",
            "         [ 5.1239,  0.0000,  0.0000],\n",
            "         [ 4.6772,  0.0000,  0.0000],\n",
            "         [ 0.6936,  0.0000,  0.0000],\n",
            "         [ 0.8863,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 3.5909,  0.0000,  0.0000],\n",
            "         [ 0.7105,  0.0000,  0.0000],\n",
            "         [ 4.9090,  0.0000,  0.0000],\n",
            "         [ 2.0285,  0.0000,  0.0000],\n",
            "         [-0.1927,  0.0000,  0.0000],\n",
            "         [-0.4260,  0.0000,  0.0000],\n",
            "         [ 0.7105,  0.0000,  0.0000]]], grad_fn=<CopySlices>)\n",
            "tensor([[[ 2.4578,  4.9156,  0.0000],\n",
            "         [ 4.9123, 12.2791,  0.0000],\n",
            "         [ 5.5499, 11.0997,  0.0000],\n",
            "         [ 5.1239, 11.3842,  0.0000],\n",
            "         [ 4.6772,  9.3545,  0.0000],\n",
            "         [ 0.6936,  1.3872,  0.0000],\n",
            "         [ 0.8863,  1.7726,  0.0000]],\n",
            "\n",
            "        [[ 3.5909,  6.7559,  0.0000],\n",
            "         [ 0.7105,  1.4209,  0.0000],\n",
            "         [ 4.9090,  9.3920,  0.0000],\n",
            "         [ 2.0285,  4.0570,  0.0000],\n",
            "         [-0.1927, -0.3853,  0.0000],\n",
            "         [-0.4260, -0.8519,  0.0000],\n",
            "         [ 0.7105,  0.9950,  0.0000]]], grad_fn=<CopySlices>)\n",
            "Epoch 1/1 - Loss: 2.6182656288146973\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "class LDPCDecoderResidual(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    LDPC Decoder with Check and Variable Layers, integrating Residual Connections and Output Mapping.\n",
        "    Includes QPSK modulation, AWGN channel simulation, and SGD-based training.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes, num_iterations, depth_L=2):\n",
        "        super(LDPCDecoderResidual, self).__init__()\n",
        "\n",
        "        self.num_nodes = num_nodes  # Number of variable nodes (messages)\n",
        "        self.num_iterations = num_iterations  # Number of iterations for decoding\n",
        "        self.depth_L = depth_L  # Number of past variable layers to store\n",
        "\n",
        "        # Trainable weights for channel reliability and residual connections\n",
        "        self.w_ch = torch.nn.Parameter(torch.ones(num_nodes))  # LLR weight\n",
        "        self.w_res = torch.nn.Parameter(torch.ones(depth_L))  # Residual connection weights\n",
        "\n",
        "        # FIFO queue for storing past variable layer outputs (max L layers)\n",
        "        self.previous_VL_storage = deque(maxlen=depth_L)\n",
        "\n",
        "    def check_layer_update(self, input_mapping_LLR, check_index_tensor):\n",
        "        \"\"\"\n",
        "        Computes Min-Sum update for Check Nodes.\n",
        "        Uses circulant shifts instead of full matrix operations for efficiency.\n",
        "        \"\"\"\n",
        "        batch_size, num_vars = input_mapping_LLR.shape\n",
        "\n",
        "        num_index_rows, num_selected_indices = check_index_tensor.shape\n",
        "\n",
        "        valid_mask = check_index_tensor != -1  # Identify valid connections\n",
        "        safe_indices = check_index_tensor.clone()\n",
        "        # print(safe_indices)\n",
        "        safe_indices[~valid_mask] = num_vars  # Replace invalid indices with safe values\n",
        "\n",
        "        # expand input vector length with zero vector\n",
        "        input_extended = torch.cat([input_mapping_LLR, torch.zeros((batch_size, 1), dtype=input_mapping_LLR.dtype)], dim=1)\n",
        "        # print(input_extended) # correct\n",
        "\n",
        "        input_expanded = input_extended.unsqueeze(0).expand(num_index_rows, -1, -1)\n",
        "        # print(input_expanded.shape)\n",
        "        # print(input_expanded)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        index_expanded = safe_indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
        "        # print(index_expanded.shape)\n",
        "        # print(index_expanded)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        # Gather elements based on circulant shift positions\n",
        "        selected_values = torch.gather(input_expanded, dim=2, index=index_expanded)\n",
        "        selected_values[~valid_mask.unsqueeze(1).expand(-1, batch_size, -1)] = 0\n",
        "\n",
        "        # Min-Sum Check Node Update\n",
        "        sign_product = torch.prod(torch.sign(selected_values), dim=1)\n",
        "        min_abs = torch.min(torch.abs(selected_values), dim=1).values\n",
        "\n",
        "        min_sum_result = sign_product * min_abs\n",
        "        check_layer_output = min_sum_result.reshape(batch_size, num_index_rows)\n",
        "        return check_layer_output\n",
        "\n",
        "    def variable_layer_update(self, input_mapping_LLR, check_to_variable_messages, variable_index_tensor, iteration):\n",
        "        \"\"\"\n",
        "        Computes the Variable Node Update in LDPC Decoding using Min-Sum with Residual Connections.\n",
        "        \"\"\"\n",
        "        batch_size, num_vars = check_to_variable_messages.shape\n",
        "        num_vars_mapped, max_neighbors = variable_index_tensor.shape\n",
        "        num_messages = check_to_variable_messages.shape[1]\n",
        "\n",
        "        valid_mask = variable_index_tensor != -1\n",
        "        safe_indices = variable_index_tensor.clone()\n",
        "        safe_indices[~valid_mask] = num_messages\n",
        "\n",
        "        extended_check_to_variable = torch.cat([\n",
        "            check_to_variable_messages,\n",
        "            torch.zeros((batch_size, 1), dtype=check_to_variable_messages.dtype)\n",
        "        ], dim=1)\n",
        "\n",
        "        check_to_variable_expanded = extended_check_to_variable.unsqueeze(0).expand(num_vars_mapped, -1, -1)\n",
        "        index_expanded = safe_indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
        "\n",
        "        # Gather and sum messages\n",
        "        gathered_messages = torch.gather(check_to_variable_expanded, dim=2, index=index_expanded)\n",
        "        gathered_messages[~valid_mask.unsqueeze(1).expand(-1, batch_size, -1)] = 0\n",
        "        summed_messages = torch.sum(gathered_messages, dim=2).T\n",
        "\n",
        "        # Compute new variable node messages\n",
        "        weighted_LLR = self.w_ch * input_mapping_LLR\n",
        "        res_contrib = torch.zeros_like(input_mapping_LLR)\n",
        "        for t in range(1, min(self.depth_L + 1, iteration + 1)):\n",
        "            res_contrib += self.w_res[t - 1] * self.previous_VL_storage[-t]\n",
        "\n",
        "        Q_new = summed_messages + weighted_LLR + res_contrib\n",
        "        self.previous_VL_storage.append(Q_new.clone())\n",
        "        return Q_new\n",
        "\n",
        "    def output_mapping(self, final_LLR, output_index_tensor):\n",
        "      \"\"\"\n",
        "      Output Mapping Layer: Converts LLRs to soft-bit probabilities and hard decision bits.\n",
        "\n",
        "      Args:\n",
        "          final_LLR (torch.Tensor): Log-Likelihood Ratios (LLRs) computed from the last iteration.\n",
        "                                    Shape: (batch_size, num_nodes)\n",
        "          output_index_tensor (torch.Tensor): Index mapping for which LLRs contribute to each output node.\n",
        "                                              Shape: (num_output_nodes, num_inputs_per_output)\n",
        "\n",
        "      Returns:\n",
        "          hard_bits (torch.Tensor): Hard decision bits (0 or 1) after thresholding LLRs.\n",
        "          soft_bits (torch.Tensor): Soft-bit probabilities (sigmoid applied LLRs).\n",
        "      \"\"\"\n",
        "\n",
        "      # Step 1: Retrieve batch size and number of nodes from the final LLR shape\n",
        "      batch_size, num_nodes = final_LLR.shape\n",
        "\n",
        "      # Step 2: Retrieve number of output nodes and number of LLRs mapped to each output node\n",
        "      num_output_nodes, num_inputs_per_output = output_index_tensor.shape\n",
        "      # print(output_index_tensor) this is correct\n",
        "\n",
        "      # Step 3: Create a mask to identify valid indices (ignore -1 entries, which represent no connection)\n",
        "      valid_mask = output_index_tensor != -1\n",
        "\n",
        "      # Step 4: Clone output index tensor and replace all -1 values with a safe index (out of range)\n",
        "      safe_indices = output_index_tensor.clone()\n",
        "      safe_indices[~valid_mask] = num_nodes  # Assigns invalid indices to a safe out-of-range value\n",
        "\n",
        "      # Step 5: Extend `final_LLR` by adding an extra column of zeros\n",
        "      # This allows safe gathering for out-of-range indices assigned in the previous step\n",
        "      extended_LLR = torch.cat([final_LLR, torch.zeros((batch_size, 1), dtype=final_LLR.dtype)], dim=1)\n",
        "\n",
        "      # Step 6: Expand the `extended_LLR` tensor for gathering\n",
        "      # Expanding ensures that we process all output nodes simultaneously\n",
        "      final_LLR_expanded = extended_LLR.unsqueeze(0).expand(num_output_nodes, -1, -1)\n",
        "\n",
        "      # Step 7: Expand `safe_indices` to match batch size\n",
        "      index_expanded = safe_indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
        "\n",
        "      # Step 8: Gather LLR values based on the index mapping\n",
        "      # This extracts the LLRs that contribute to each output node\n",
        "      gathered_outputs = torch.gather(final_LLR_expanded, dim=2, index=index_expanded)\n",
        "\n",
        "      # Get unique indices\n",
        "      unique_labels = torch.unique(output_index_tensor)\n",
        "\n",
        "      # print(\"inputs\")\n",
        "      # print(gathered_outputs)\n",
        "      # print(\"\\nvalues\")\n",
        "      # print(output_index_tensor)\n",
        "\n",
        "      # Group values and compute column-wise mean for each group\n",
        "      grouped_means = [gathered_outputs[0][:, output_index_tensor[0] == label].mean(dim=1, keepdim=True) for label in unique_labels]\n",
        "\n",
        "      # Stack results into shape [4, 2, 1]\n",
        "      aggregated_LLR = torch.stack(grouped_means)  # Shape [4, 2, 1]\n",
        "\n",
        "      # Remove last dimension (squeeze) and transpose to get shape [2, 4]\n",
        "      aggregated_LLR = aggregated_LLR.squeeze(-1).T\n",
        "\n",
        "      # Step 13: Compute soft-bit probabilities using the sigmoid function\n",
        "      soft_bits = torch.sigmoid(aggregated_LLR)  # Converts LLRs to probabilities (range: 0 to 1)\n",
        "\n",
        "      # Step 14: Compute hard decision bits based on LLR thresholding\n",
        "      hard_bits = (aggregated_LLR <= 0).int()  # If LLR ≤ 0 → Decoded bit = 1, Else → 0\n",
        "\n",
        "      # Step 15: Return both hard decision bits and soft-bit probabilities\n",
        "      return hard_bits, soft_bits\n",
        "\n",
        "\n",
        "    def forward(self, input_mapping_LLR, output_index_tensor, check_index_tensor, var_index_tensor):\n",
        "        \"\"\" Full forward pass through LDPC decoding layers. \"\"\"\n",
        "        batch_size = input_mapping_LLR.shape[0]\n",
        "        Q = torch.zeros(batch_size, self.num_nodes, self.num_iterations, device=input_mapping_LLR.device)\n",
        "        print(Q)\n",
        "\n",
        "        for l in range(self.num_iterations):\n",
        "            check_messages = self.check_layer_update(Q[:, :, l - 1] if l > 0 else input_mapping_LLR, check_index_tensor)\n",
        "\n",
        "            if l == self.num_iterations - 1:\n",
        "                return self.output_mapping(check_messages, output_index_tensor)\n",
        "\n",
        "            Q[:, :, l] = self.variable_layer_update(input_mapping_LLR, check_messages, var_index_tensor, l)\n",
        "            print(Q)\n",
        "\n",
        "def generate_zero_codewords(batch_size, num_bits):\n",
        "    \"\"\"\n",
        "    Generates two zero codewords (before modulation).\n",
        "\n",
        "    Args:\n",
        "        num_bits (int): Number of bits per codeword.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape (2, num_bits) representing two zero codewords.\n",
        "    \"\"\"\n",
        "    # Create two rows of all-zero bits\n",
        "    codewords = torch.zeros((batch_size, num_bits), dtype=torch.int64)\n",
        "    return codewords\n",
        "\n",
        "\n",
        "def qpsk_modulate(bit_sequences):\n",
        "    \"\"\"\n",
        "    Maps a batch of bit sequences to QPSK symbols.\n",
        "\n",
        "    Args:\n",
        "        bit_sequences (torch.Tensor): Shape [batch_size, num_bits], containing {0,1}.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Shape [batch_size, num_symbols] containing QPSK symbols.\n",
        "    \"\"\"\n",
        "    batch_size, num_bits = bit_sequences.shape\n",
        "\n",
        "    # Ensure even number of bits (pad if necessary)\n",
        "    if num_bits % 2 != 0:\n",
        "        bit_sequences = torch.cat([bit_sequences, torch.zeros((batch_size, 1), dtype=torch.int64)], dim=1)\n",
        "\n",
        "    # Reshape to bit pairs for QPSK mapping\n",
        "    bit_pairs = bit_sequences.view(batch_size, -1, 2)  # Shape: [batch_size, num_symbols, 2]\n",
        "\n",
        "    # QPSK Mapping (Gray-coded)\n",
        "    mapping = {\n",
        "        (0, 0): complex(1, 1),   # +1 + j1\n",
        "        (0, 1): complex(1, -1),  # +1 - j1\n",
        "        (1, 0): complex(-1, 1),  # -1 + j1\n",
        "        (1, 1): complex(-1, -1)  # -1 - j1\n",
        "    }\n",
        "\n",
        "    # Convert bit pairs to QPSK symbols\n",
        "    qpsk_symbols = torch.tensor(\n",
        "        [[mapping[tuple(bits.tolist())] for bits in sample] for sample in bit_pairs]\n",
        "    )\n",
        "\n",
        "    return qpsk_symbols  # Shape: [batch_size, num_symbols]\n",
        "\n",
        "def awgn_channel(signal, snr_db):\n",
        "    \"\"\"\n",
        "    Simulates an AWGN channel by adding noise to the input signal.\n",
        "\n",
        "    Args:\n",
        "        signal (torch.Tensor): QPSK symbols.\n",
        "        snr_db (float): Signal-to-Noise Ratio in dB.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Noisy received symbols.\n",
        "    \"\"\"\n",
        "    snr_linear = 10 ** (snr_db / 10)  # Convert SNR from dB to linear scale\n",
        "\n",
        "    # noise = noise_std * (torch.randn_like(signal) + 1j * torch.randn_like(signal))  # Generate complex noise\n",
        "\n",
        "    # Compute noise standard deviation (corrected)\n",
        "    noise_std = torch.sqrt(torch.tensor(1 / (2 * snr_linear), dtype=signal.dtype))\n",
        "\n",
        "    # Generate correct complex Gaussian noise (independent real & imaginary parts)\n",
        "    noise = noise_std * (torch.randn_like(signal.real) + 1j * torch.randn_like(signal.imag))\n",
        "\n",
        "    # Add noise to QPSK symbols\n",
        "    received_signal = signal + noise\n",
        "\n",
        "    return received_signal\n",
        "\n",
        "def qpsk_demodulate(received_signal, snr_db):\n",
        "    \"\"\"\n",
        "    QPSK Demodulation: Converts received symbols to Log-Likelihood Ratios (LLRs) for multiple batches.\n",
        "\n",
        "    Args:\n",
        "        received_signal (torch.Tensor): Noisy QPSK symbols of shape (batch_size, num_symbols), complex values.\n",
        "        snr_db (float): Signal-to-Noise Ratio in dB.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: LLR values of shape (batch_size, num_symbols * 2), where each symbol produces 2 LLRs.\n",
        "    \"\"\"\n",
        "    # Convert SNR from dB to linear scale\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "\n",
        "    # Compute noise variance from the linear SNR value\n",
        "    noise_var = 1 / (2 * snr_linear)\n",
        "\n",
        "    # Compute LLR values for the real part (first bit in QPSK symbol)\n",
        "    llr_real = 2 * received_signal.real / noise_var  # Shape: (batch_size, num_symbols)\n",
        "\n",
        "    # Compute LLR values for the imaginary part (second bit in QPSK symbol)\n",
        "    llr_imag = 2 * received_signal.imag / noise_var  # Shape: (batch_size, num_symbols)\n",
        "\n",
        "    # Concatenate real and imaginary LLRs along the last dimension\n",
        "    llr_output = torch.cat((llr_real, llr_imag), dim=-1)  # Shape: (batch_size, num_symbols * 2)\n",
        "\n",
        "    return llr_output\n",
        "\n",
        "# Training function\n",
        "def train_decoder(decoder, num_epochs, learning_rate, variable_bit_length):\n",
        "    \"\"\"\n",
        "    Trains the LDPC decoder using Stochastic Gradient Descent (SGD).\n",
        "    Args:\n",
        "        decoder (LDPCDecoderResidual): The LDPC decoder model.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "        None (Prints training loss at each epoch).\n",
        "    \"\"\"\n",
        "    # Initialize the optimizer for training using SGD\n",
        "    optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loop over each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Generate a random SNR value (in dB) between -4 dB and 0 dB\n",
        "        snr_db = torch.FloatTensor(1).uniform_(-4, 0).item()\n",
        "\n",
        "        # # Generate an all-zero codeword (size: 2 * num_nodes because QPSK maps 2 bits per symbol)\n",
        "        messages = generate_zero_codewords(batch_size, variable_bit_length)\n",
        "        # print(\"Line 302:\")\n",
        "        # print(messages)\n",
        "\n",
        "        # Modulate input LLR using QPSK\n",
        "        qpsk_symbols = qpsk_modulate(messages)\n",
        "\n",
        "        # Pass the QPSK symbols through an AWGN channel with the selected SNR\n",
        "        received_signal = awgn_channel(qpsk_symbols, snr_db)\n",
        "\n",
        "        # Demodulate the received signal to obtain LLRs\n",
        "        llrs = qpsk_demodulate(received_signal, snr_db)\n",
        "\n",
        "        # Copy LLRs depending on the H matrix observed\n",
        "        copied_LLR = llrs[:, indices[0]]\n",
        "        # print(copied_LLR)\n",
        "\n",
        "        # Forward pass: Decode the received LLR values using the LDPC decoder\n",
        "        hard_bits, soft_bits = decoder(copied_LLR, output_index_tensor, check_index_tensor, var_index_tensor)\n",
        "\n",
        "        # Compute loss using Binary Cross-Entropy (BCE)\n",
        "        # The target is the original transmitted bits converted to float\n",
        "        target = messages.float()\n",
        "        loss = F.binary_cross_entropy(soft_bits, target)\n",
        "\n",
        "        # Zero gradients before backpropagation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters using SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item()}\")\n",
        "        # print(f\"Input LLR: {llrs}\")\n",
        "        # print(f\"Soft Bits:\\n{soft_bits}\\n\")\n",
        "\n",
        "# variable bit length\n",
        "n = H.shape[1] # number of cols = number of var bits\n",
        "# print(n)\n",
        "\n",
        "# print(f\"Our input:\\n{input_mapping_LLR}\\n\")\n",
        "decoder = LDPCDecoderResidual(num_nodes=7, num_iterations=3, depth_L=2)\n",
        "train_decoder(decoder, num_epochs=1, learning_rate=0.01, variable_bit_length=n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEjaCihYBIQB"
      },
      "source": [
        "# Used for testing 5G LDPC codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "rmmFMV6wag1N",
        "outputId": "6611675c-0dcd-4744-a685-f2732cc80e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([42, 52])\n",
            "tensor([[ 1.,  1.,  0.,  ..., -1., -1., -1.],\n",
            "        [ 3., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [ 1.,  2., -1.,  ..., -1., -1., -1.],\n",
            "        ...,\n",
            "        [ 3., -1., -1.,  ...,  0., -1., -1.],\n",
            "        [-1., -1.,  0.,  ..., -1.,  0., -1.],\n",
            "        [-1.,  1., -1.,  ..., -1., -1.,  0.]])\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'qpsk_modulate' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-88735fcd1fdb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Run simulations for both short and long LDPC codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mber_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfer_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_ldpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mber_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfer_long\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_ldpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-88735fcd1fdb>\u001b[0m in \u001b[0;36mevaluate_ldpc\u001b[0;34m(H, snr_range, num_trials)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Modulate using QPSK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mqpsk_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqpsk_modulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransmitted_bits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Pass through AWGN channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'qpsk_modulate' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def evaluate_ldpc(H, snr_range, num_trials=100):\n",
        "    \"\"\"\n",
        "    Evaluates BER and FER for a given LDPC parity-check matrix over an SNR range.\n",
        "\n",
        "    Args:\n",
        "        H (torch.Tensor): Parity-check matrix.\n",
        "        snr_range (list): List of SNR values to test.\n",
        "        num_trials (int): Number of trials per SNR.\n",
        "\n",
        "    Returns:\n",
        "        (list, list): BER and FER values for each SNR.\n",
        "    \"\"\"\n",
        "    ber_results = []\n",
        "    fer_results = []\n",
        "\n",
        "    for snr_db in snr_range:\n",
        "        total_ber = 0\n",
        "        total_fer = 0\n",
        "\n",
        "        for _ in range(num_trials):\n",
        "            # Generate all-zero codeword\n",
        "            transmitted_bits = torch.zeros((1, H.shape[1]), dtype=torch.int64)\n",
        "\n",
        "            # Modulate using QPSK\n",
        "            qpsk_symbols = qpsk_modulate(transmitted_bits.view(-1))\n",
        "\n",
        "            # Pass through AWGN channel\n",
        "            received_signal = awgn_channel(qpsk_symbols, snr_db)\n",
        "\n",
        "            # Demodulate to LLRs\n",
        "            llrs = qpsk_demodulate(received_signal, snr_db)\n",
        "\n",
        "            # Decode using LDPC\n",
        "            decoded_bits, _ = decoder(llrs, output_index_tensor, check_index_tensor, var_index_tensor)\n",
        "\n",
        "            # Compute BER and FER\n",
        "            ber, fer = compute_ber_fer(transmitted_bits, decoded_bits)\n",
        "            total_ber += ber\n",
        "            total_fer += fer\n",
        "\n",
        "        ber_results.append(total_ber / num_trials)\n",
        "        fer_results.append(total_fer / num_trials)\n",
        "\n",
        "    return ber_results, fer_results\n",
        "\n",
        "# Define SNR range\n",
        "snr_range = list(range(-4, 6))\n",
        "\n",
        "# Long & Short 5G LDPC Codes\n",
        "with open('/content/NR_2_0_4.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Parse each line into a list of numbers\n",
        "H_short = []\n",
        "for line in lines:\n",
        "    # Split on whitespace and convert each piece to float (or int)\n",
        "    row = [float(x) for x in line.split()]\n",
        "    H_short.append(row)\n",
        "H_short = torch.tensor(H_short)\n",
        "print(H_short.shape)\n",
        "print(H_short)\n",
        "\n",
        "# H_long =\n",
        "\n",
        "# Run simulations for both short and long LDPC codes\n",
        "ber_short, fer_short = evaluate_ldpc(H_short, snr_range)\n",
        "ber_long, fer_long = evaluate_ldpc(H_long, snr_range)\n",
        "\n",
        "# Plot BER vs. SNR\n",
        "plt.plot(snr_range, ber_short, label=\"Short LDPC\")\n",
        "plt.plot(snr_range, ber_long, label=\"Long LDPC\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"BER\")\n",
        "plt.legend()\n",
        "plt.title(\"BER vs. SNR\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWnETJqHKly-"
      },
      "source": [
        "# Testing the base graph extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hui3kPQdKs34",
        "outputId": "dd24663d-6e02-4fc4-df81-80b69d1bba56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final LLRs after 1 iteration: [ 0.01361436 -1.217307    1.8415205  -0.84667087 -0.5372044   1.7802583 ]\n",
            "Step 0, Loss=24.8701\n",
            "Step 1, Loss=24.8522\n",
            "Step 2, Loss=24.8343\n",
            "Step 3, Loss=24.8164\n",
            "Step 4, Loss=24.7985\n",
            "Step 5, Loss=24.7807\n",
            "Step 6, Loss=24.7628\n",
            "Step 7, Loss=24.7450\n",
            "Step 8, Loss=24.7272\n",
            "Step 9, Loss=24.7093\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------\n",
        "# 1) DEFINE BASE GRAPH\n",
        "# -------------------------\n",
        "# Let's say we have a small base graph with 2 rows (check nodes) and 3 columns (variable nodes).\n",
        "# SHIFT[r,c] = -1 means NO edges; otherwise SHIFT[r,c] = an integer shift.\n",
        "# We choose a small example for demonstration:\n",
        "\n",
        "BG = [\n",
        "    [ 0,  1, -1],\n",
        "    [-1,  0,  2]\n",
        "]\n",
        "R_BG = len(BG)       # 2\n",
        "C_BG = len(BG[0])    # 3\n",
        "\n",
        "# Lifting factor\n",
        "Z = 2  # So final parity-check matrix is (2*Z) x (3*Z) = 4 x 6\n",
        "\n",
        "# -------------------------\n",
        "# 2) NEURAL MODULE FOR PARAMETER TYING\n",
        "# -------------------------\n",
        "# We'll create a simple \"per-base-graph-cell\" neural transform (a single Linear layer)\n",
        "# that we will apply to the messages for edges coming from that cell.\n",
        "\n",
        "class CellTransform(nn.Module):\n",
        "    def __init__(self, in_features=1, out_features=1):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (any_batch, in_features)\n",
        "        return self.linear(x)\n",
        "\n",
        "class TiedNeuralLDPCDecoder(nn.Module):\n",
        "    def __init__(self, base_graph, Z):\n",
        "        super().__init__()\n",
        "        self.base_graph = base_graph\n",
        "        self.Z = Z\n",
        "\n",
        "        # Build a small nn.ModuleDict for each cell that has shift >= 0\n",
        "        # Key: (r,c)\n",
        "        # Value: a CellTransform (or could be a small MLP)\n",
        "        self.cell_modules = nn.ModuleDict()\n",
        "\n",
        "        for r in range(len(base_graph)):\n",
        "            for c in range(len(base_graph[0])):\n",
        "                if base_graph[r][c] >= 0:\n",
        "                    key = f\"({r},{c})\"\n",
        "                    self.cell_modules[key] = CellTransform(in_features=1, out_features=1)\n",
        "\n",
        "    def forward(self, channel_llrs):\n",
        "        \"\"\"\n",
        "        channel_llrs: a tensor of shape [C_BG * Z], i.e. for each variable node in the expanded domain,\n",
        "                      we have 1 LLR. (This simple example uses 1D for demonstration.)\n",
        "\n",
        "        We'll do ONE iteration of (check-update -> variable-update) in a naive way.\n",
        "        \"\"\"\n",
        "        # --------------------------------\n",
        "        #  CHECK NODE UPDATE (Min-Sum-ish)\n",
        "        # --------------------------------\n",
        "        # We'll store check->variable messages in a tensor of same shape [C_BG*Z],\n",
        "        # but we must gather them for each check node (r,z_r) -> variable node (c,z_c).\n",
        "        # For simplicity, let's store a dictionary: key=(r,z_r,c,z_c), value=message\n",
        "        M_c_to_v = {}\n",
        "\n",
        "        # For each check node in expanded domain: (r in [0..R_BG-1], z_r in [0..Z-1])\n",
        "        for r in range(len(self.base_graph)):\n",
        "            for z_r in range(self.Z):\n",
        "\n",
        "                # Collect all edges from (r,z_r) => (c,z_c), if SHIFT[r,c] != -1\n",
        "                # Condition: z_r = (z_c + shift[r][c]) mod Z\n",
        "                for c in range(len(self.base_graph[r])):\n",
        "                    shift_val = self.base_graph[r][c]\n",
        "                    if shift_val >= 0:\n",
        "                        # possible edges\n",
        "                        for z_c in range(self.Z):\n",
        "                            if z_r == (z_c + shift_val) % self.Z:\n",
        "                                # This means check node (r,z_r) connects to variable node (c,z_c).\n",
        "                                # We need the incoming variable->check message. In this simple example,\n",
        "                                # let's approximate the variable->check message as the channel_llr\n",
        "                                # from that variable node (no prior iteration).\n",
        "\n",
        "                                # channel_llrs index is c*Z + z_c\n",
        "                                v2c_message = channel_llrs[c*self.Z + z_c].view(1,1)  # shape [1,1]\n",
        "\n",
        "                                # Pass through the \"check transform\" for this cell = (r,c).\n",
        "                                # Or do a simple \"min-sum\" style. We'll illustrate a trivial transform:\n",
        "                                key_module = f\"({r},{c})\"\n",
        "                                module = self.cell_modules[key_module]\n",
        "                                c2v_message = module(v2c_message)  # shape [1,1]\n",
        "\n",
        "                                # For a real check node update, you'd combine messages from ALL other variable nodes,\n",
        "                                # but let's keep it very minimal.\n",
        "\n",
        "                                M_c_to_v[(r,z_r,c,z_c)] = c2v_message\n",
        "                    else:\n",
        "                        # shift_val == -1 => no edges\n",
        "                        pass\n",
        "\n",
        "        # --------------------------------\n",
        "        #  VARIABLE NODE UPDATE\n",
        "        # --------------------------------\n",
        "        # Similarly, each variable node (c,z_c) will combine incoming check->var messages from all checks\n",
        "        # that connect to it. We'll do a naive sum + channel prior, then a small transform.\n",
        "\n",
        "        # We'll store new variable->check messages. For demonstration, let's just produce final LLRs.\n",
        "        final_llrs = torch.zeros_like(channel_llrs)\n",
        "\n",
        "        for c in range(C_BG):\n",
        "            for z_c in range(self.Z):\n",
        "                # Identify which checks (r,z_r) connect here:\n",
        "                # Condition: z_r = (z_c + shift[r,c]) mod Z\n",
        "                # We'll sum up all M_c_to_v[r,z_r,c,z_c] that exist.\n",
        "\n",
        "                sum_msg = channel_llrs[c*self.Z + z_c].clone()  # start with channel prior\n",
        "                # We'll do a naive sum of all check->var messages for demonstration\n",
        "                for r in range(R_BG):\n",
        "                    shift_val = self.base_graph[r][c]\n",
        "                    if shift_val >= 0:\n",
        "                        # find z_r that connects:\n",
        "                        z_r = (z_c + shift_val) % self.Z\n",
        "                        if (r,z_r,c,z_c) in M_c_to_v:\n",
        "                            sum_msg += M_c_to_v[(r,z_r,c,z_c)].squeeze()  # shape -> scalar\n",
        "                final_llrs[c*self.Z + z_c] = sum_msg\n",
        "\n",
        "        return final_llrs\n",
        "\n",
        "# -------------------------\n",
        "# 3) DEMO USAGE\n",
        "# -------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create our model\n",
        "    decoder = TiedNeuralLDPCDecoder(BG, Z)\n",
        "\n",
        "    # Example channel LLRs for the expanded code (C_BG*Z = 3*2=6 variables)\n",
        "    # Let's say we have some random LLRs:\n",
        "    channel_llrs = torch.tensor([ 0.8, -1.2, 1.5, 0.2, -0.7, 0.9 ], dtype=torch.float)\n",
        "\n",
        "    # Forward pass (one iteration)\n",
        "    final_llrs = decoder(channel_llrs)\n",
        "    print(\"Final LLRs after 1 iteration:\", final_llrs.detach().numpy())\n",
        "\n",
        "    # Example of a training step:\n",
        "    # Suppose we have some ground-truth bits we want to recover.\n",
        "    # We'll do a simple supervised loss (e.g., MSE on LLR vs. desired sign).\n",
        "    # This is purely illustrative.\n",
        "\n",
        "    optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "    target_bits = torch.tensor([0,1,0,0,1,1], dtype=torch.float)  # 0 or 1\n",
        "    # Convert bits to \"ideal\" LLR sign: + for bit=0, - for bit=1\n",
        "    desired_llrs = 5.0*(1.0 - 2.0*target_bits)  #  bit0 => +5.0, bit1 => -5.0\n",
        "\n",
        "    for step in range(10):\n",
        "        optimizer.zero_grad()\n",
        "        out_llrs = decoder(channel_llrs)\n",
        "        loss = F.mse_loss(out_llrs, desired_llrs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Step {step}, Loss={loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWYbYekjnb87"
      },
      "source": [
        "# 5G LDPC CODES, can use ✅\n",
        "https://github.com/manuts/NR-LDPC-BG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igyqVDWfsPBX"
      },
      "source": [
        "# 1.1 Find Long-length 5G LDPC code 🔴 (finish by 21 Feb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT3rudrVo-jO",
        "outputId": "6479c3d6-00a9-4945-c990-f479dfbd8b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    0   1   2   3   4   5   6   7   8   9   ...  58  59  60  61  62  63  64  \\\n",
            "0    1  -1   2   1  -1  -1   1   2   1   0  ...  -1   1  -1   3  -1  -1  -1   \n",
            "1    0   2   2   3  -1   2  -1   0   3   3  ...   2   0   1  -1   2   1  -1   \n",
            "2    0  -1   2   2   1   1  -1  -1   1   1  ...   2   3   0  -1   1   0  -1   \n",
            "3   -1  -1   1   1   2   2   3   2   2   1  ...   0   1   2   3   0   3   3   \n",
            "4    2  -1  -1  -1   1   0   0   1   1  -1  ...  -1   2   0  -1   0   3   1   \n",
            "5   -1   1   0  -1  -1   2   1   0   2  -1  ...   0   1   2   1   3  -1   3   \n",
            "6    2   1   1   1   0   0   0   1   0  -1  ...   1   1   1   0   2   0   0   \n",
            "7   -1   0   3   3   0   0   2   1  -1   0  ...  -1   0   2   3   0   1   3   \n",
            "8   -1   1   3   0   2   2  -1   3   0   2  ...   0   1  -1  -1   3   0  -1   \n",
            "9    0   0   3   2   3  -1   3   3   1   3  ...   2   3   3   3   2   0   2   \n",
            "10   0   1   2   2   3  -1  -1  -1   1   0  ...   1   3   2   0   1   3   1   \n",
            "11   2   3   1   2   1   2  -1  -1  -1  -1  ...   1  -1   3   3   3   3   3   \n",
            "12   1   3   0   2   0   2   0   3  -1   1  ...  -1   2   2   1   3   1  -1   \n",
            "13   2  -1   2   2   1   0  -1  -1   0   3  ...  -1   3  -1   3   2   2   3   \n",
            "14  -1   3   2  -1   3   3   3  -1  -1   2  ...   2   3   1   1  -1  -1   3   \n",
            "15  -1   3   0   3   0   0   1   3  -1   2  ...   3   2   1   1   2   2  -1   \n",
            "16  -1   0   3   2  -1   0   1   0   3   3  ...   2   1   0   0   0   3   2   \n",
            "17   3   0   0   3   0   3   0  -1   2   0  ...   2   1  -1   1  -1  -1   0   \n",
            "18   2   0   0   0   3   1   3   2   1   1  ...  -1   0   0   1   3   3   1   \n",
            "19   1  -1   0   1   0   0   2   2   2   0  ...   3   2   2   2   3   0   1   \n",
            "20   3  -1   0  -1  -1   0   3   1   2   0  ...   0   0   2   1   0   2   3   \n",
            "21   0   1  -1   1   3   1   2   1  -1   0  ...  -1   1   2   2   1   3  -1   \n",
            "22   2   3   0   2   3  -1   3   3   2  -1  ...   0   0  -1  -1  -1   2   0   \n",
            "23   1   0   3  -1   2   3   3   2  -1   0  ...   1  -1   1   2   3  -1   1   \n",
            "24   2  -1   2   0  -1  -1   1  -1   2   2  ...  -1   3   3   0   1   3   3   \n",
            "25   2   1  -1   3   0   1   3   0   1   0  ...   2  -1   3   1   2   0   1   \n",
            "26  -1   0   0   3   2   1   3   1  -1  -1  ...   0   2  -1   3   2   1   1   \n",
            "27   2   3   1   0   1   1  -1   3   2   0  ...   3   1   0  -1  -1   1   0   \n",
            "28   3   0   3   2   1   1  -1  -1  -1   3  ...   0   1   3   1   2  -1  -1   \n",
            "29   3   3   1   0  -1   0  -1  -1   2   2  ...   0   2   0   1   0   1   1   \n",
            "30   0   2   1   0   3   0   3   0   3   0  ...  -1   0   1  -1   1   1   3   \n",
            "31  -1   2  -1   2   2  -1   3   3   1  -1  ...   3   1   0   3   1   0   1   \n",
            "32   0   0  -1   0   1   2  -1  -1   1   2  ...  -1   1   3   0   2   3  -1   \n",
            "33   2   3   3   0  -1   1   2   3   3   1  ...   2   1   1   0   3  -1   1   \n",
            "34   0   1   0   0   3   2   2   0   2   1  ...   0   1   0   0   1   2  -1   \n",
            "35   1  -1  -1   3   1  -1   2  -1   0   1  ...   2  -1   2   1   1   1   0   \n",
            "36   0   2   1  -1   2   1   2  -1   2   3  ...   3   0   0  -1   1   3   3   \n",
            "37   1   3   3   2   1   3   0   3  -1   3  ...   3   3  -1   2   0   2   2   \n",
            "38  -1   1   3  -1  -1   2   0  -1   1   1  ...   1   1   0   3   1   0  -1   \n",
            "39   0   0   1   0   2   0   2  -1  -1   1  ...   2   1  -1   1   0  -1   0   \n",
            "40   1  -1   2   0   1   1   3   1   3   2  ...   2   3   2   2   2   0  -1   \n",
            "41   2  -1   0   3   1   2   3   2   3  -1  ...  -1   1   0   1   3   0   3   \n",
            "42   1  -1   3  -1   3   1   3   2   2   0  ...   2   1   0   1   3   1   2   \n",
            "43   3  -1   0   1  -1  -1  -1  -1   2  -1  ...   3   0  -1   0  -1   3   2   \n",
            "44   2   2   1   0   0   0   0   1  -1   3  ...   2   2  -1   3  -1   3   1   \n",
            "45   1  -1   2   1   1  -1  -1   2   0   2  ...   2  -1   2  -1   0   3   3   \n",
            "\n",
            "    65  66  67  \n",
            "0    0   2   0  \n",
            "1    1   1   0  \n",
            "2    3   1   2  \n",
            "3    3   1  -1  \n",
            "4    2   2   3  \n",
            "5    2  -1   2  \n",
            "6    2   1   3  \n",
            "7   -1  -1   1  \n",
            "8    2   0   0  \n",
            "9    0   0   1  \n",
            "10  -1   0   1  \n",
            "11   1   1  -1  \n",
            "12   1   2   0  \n",
            "13   0   3   1  \n",
            "14   1   2   1  \n",
            "15   1   2   0  \n",
            "16   3   0   2  \n",
            "17   1   3   3  \n",
            "18   3   0   0  \n",
            "19   2  -1   0  \n",
            "20   3   0   1  \n",
            "21   0   0   0  \n",
            "22   0  -1   0  \n",
            "23   2   2   1  \n",
            "24   3   3   2  \n",
            "25   2  -1   2  \n",
            "26   3  -1   0  \n",
            "27   1   1   3  \n",
            "28   1   2   1  \n",
            "29   3  -1   3  \n",
            "30   1   0   1  \n",
            "31   0   2  -1  \n",
            "32   0  -1   0  \n",
            "33   1   1   0  \n",
            "34   2  -1   2  \n",
            "35  -1   2   1  \n",
            "36  -1   3   1  \n",
            "37   3  -1   1  \n",
            "38   3   3   0  \n",
            "39   2   0  -1  \n",
            "40   2   0   3  \n",
            "41   2  -1   2  \n",
            "42   3  -1   0  \n",
            "43   0   0   3  \n",
            "44  -1  -1   3  \n",
            "45   1  -1   1  \n",
            "\n",
            "[46 rows x 68 columns]\n",
            "    0   1   2   3   4   5   6   7   8   9   ...  42  43  44  45  46  47  48  \\\n",
            "0   -1   2  -1   0   0   1   2   2  -1   2  ...   1   1   3   0   0   3  -1   \n",
            "1    2  -1   3   0   0  -1   3   3   0   0  ...  -1  -1  -1   0  -1   1  -1   \n",
            "2    0   0   1  -1   0   0   3   1   3  -1  ...   3   0   3   0   3   1   1   \n",
            "3   -1   3   0   1   1   1   2   1   2  -1  ...  -1   1  -1   3   1  -1   1   \n",
            "4    2  -1  -1   0   2   0   0   1   0   0  ...   0   2   1   3   1   3   1   \n",
            "5    2   0   1   1   1  -1  -1  -1  -1   0  ...   1   0  -1   1   0   1   1   \n",
            "6    1   2  -1   1   2   1   1   3  -1  -1  ...   1   2  -1  -1   3   2   0   \n",
            "7   -1   0   3   1   2   0   1   1   1   2  ...   3   2   2   3   3   2   1   \n",
            "8    0   2   2   0   2  -1   1  -1   0   3  ...   1  -1  -1   2   2   0   1   \n",
            "9    1   0   0   2   2   2   0  -1  -1   3  ...   1   3   1   2   1   1   2   \n",
            "10   2   1   0   1   3   3   3  -1   0   0  ...   3   0   0   1   2   3  -1   \n",
            "11   3   0   3   0  -1   3   1   3   1   2  ...   3   1   2  -1   3   0   0   \n",
            "12   0   3   1   2   2   0   2   2   2   3  ...   1   2   2   2   0   0   1   \n",
            "13  -1  -1   3   1  -1   2   2  -1   3  -1  ...   0  -1   3   0  -1   1   2   \n",
            "14   3  -1   1   1   2   2   2   0   2   0  ...   2   2   3   0   3   3   0   \n",
            "15  -1   2   0   0   1   1   2   1   2   2  ...   1   2   1   0   2   1  -1   \n",
            "16   1   0   1   3  -1   1  -1   0   2   1  ...   1  -1   1  -1   2   0   0   \n",
            "17  -1  -1   2  -1   0   1   2  -1   0  -1  ...  -1   3   2  -1   1   3   2   \n",
            "18   1   0   1   1  -1   0   0   2   2   0  ...   2   2   0   0   0   0   3   \n",
            "19   1   2   2   2   2   0  -1   0   0   3  ...   3   1  -1   1   1  -1   2   \n",
            "20   3   1   0   0   2  -1   0  -1   3   2  ...   3   0  -1   2   2   2   0   \n",
            "21   2   2  -1   3  -1   2   2   3   0   0  ...   0   1   3   2   2   2   2   \n",
            "22   2   1   2   3   3   2  -1   0  -1   0  ...   3   1   3   2  -1   3   2   \n",
            "23   2   3  -1   1   0  -1   0   3   1   2  ...   1   3   1   0   3  -1   2   \n",
            "24  -1   1   1  -1   3  -1   1   1   2   0  ...   3  -1   1  -1   3  -1   0   \n",
            "25   2   0  -1   3  -1   0   3  -1   1   3  ...   0   1   2   3   1   2  -1   \n",
            "26   1   1   2   3   2   3   0   1  -1   3  ...   1   2  -1  -1   0   3   1   \n",
            "27   0   2   3   2   2   0   2   2  -1   1  ...   2  -1  -1   0   0   1  -1   \n",
            "28   2   2   1   0   2   0   2  -1   2   1  ...  -1   2   1   1   0  -1  -1   \n",
            "29   1   1   2  -1   2   2   2   3   0   1  ...  -1   2   3   3  -1   1   1   \n",
            "30   2   1  -1   0   0   0   0   1   3   0  ...   1   2   3   2   3   0   3   \n",
            "31   0   0   0   1   0   2   1   0   0   1  ...   0  -1  -1   3   2   0   2   \n",
            "32   2   0   0   1  -1   3   3   0  -1   1  ...   3   1  -1   3  -1   1   0   \n",
            "33  -1   0  -1   0   3   3   1   3   1   0  ...   1   2   1  -1   0   3   2   \n",
            "34   2   1   2  -1   3  -1   0  -1   3   2  ...   3   3   2   1   3   1  -1   \n",
            "35   2   0   2   1   1   1   1  -1   0   1  ...   0   1   3   1   2   0   0   \n",
            "36  -1   0   0   2  -1  -1  -1   2   2   0  ...   0   1   1  -1   0  -1   1   \n",
            "37   1   0   0  -1   3   3   2   3   2   1  ...  -1  -1   2   0   1   2   0   \n",
            "38   3   0   2   2  -1   1   0   3   0  -1  ...   3   1   1   3  -1   1   2   \n",
            "39   2   2   0  -1  -1  -1   2   1   0   1  ...   0   1  -1   1   3   0   1   \n",
            "40   2   3   1   1   3  -1  -1   0   2   3  ...   1   1   1   2   3   0   1   \n",
            "41   3   3   3   2   2   3   0   0  -1  -1  ...   3   2   2   0   2  -1   1   \n",
            "\n",
            "    49  50  51  \n",
            "0    0   2   2  \n",
            "1    0   1   3  \n",
            "2   -1   3   3  \n",
            "3    3   0  -1  \n",
            "4    0   3   2  \n",
            "5    0   0   1  \n",
            "6   -1   2   2  \n",
            "7    3   3   0  \n",
            "8    3  -1   2  \n",
            "9   -1   2   1  \n",
            "10   2  -1   3  \n",
            "11   1   3   3  \n",
            "12  -1   3   3  \n",
            "13   0  -1  -1  \n",
            "14  -1  -1   2  \n",
            "15   0   1   2  \n",
            "16   3   2   0  \n",
            "17   2   3   0  \n",
            "18   3   1  -1  \n",
            "19   0   1   3  \n",
            "20   3   1   2  \n",
            "21  -1   2   3  \n",
            "22  -1   3   3  \n",
            "23   1   0   1  \n",
            "24   1   1   1  \n",
            "25  -1   0   1  \n",
            "26   1   2   2  \n",
            "27   0   1  -1  \n",
            "28   3   1   1  \n",
            "29   2  -1   3  \n",
            "30   0   0  -1  \n",
            "31   2   2  -1  \n",
            "32   3  -1  -1  \n",
            "33   3   3   1  \n",
            "34   1   3   3  \n",
            "35   2   3  -1  \n",
            "36   3   0   3  \n",
            "37   1  -1   0  \n",
            "38  -1  -1  -1  \n",
            "39   2   3  -1  \n",
            "40   2  -1  -1  \n",
            "41   2   0   0  \n",
            "\n",
            "[42 rows x 52 columns]\n"
          ]
        }
      ],
      "source": [
        "# Define Base Matrix 1 (BM1) - 46x68 (Simplified Example with Random Values)\n",
        "BM1 = torch.randint(-1, 4, (46, 68))  # -1 represents zero block, 0-3 are circulant shifts\n",
        "\n",
        "# Define Base Matrix 2 (BM2) - 42x52 (Simplified Example with Random Values)\n",
        "BM2 = torch.randint(-1, 4, (42, 52))  # -1 represents zero block, 0-3 are circulant shifts\n",
        "\n",
        "# Define Lifting Factors (Z) for different scenarios\n",
        "Z_values = [16, 32, 64, 128, 256]  # 5G NR standard Z values\n",
        "\n",
        "# Function to expand base matrix using a selected Z value\n",
        "def expand_base_matrix(base_matrix, Z):\n",
        "    \"\"\"\n",
        "    Expands the given base matrix by replacing each entry with a ZxZ block.\n",
        "    - A zero block (-1 in base matrix) becomes a ZxZ zero matrix.\n",
        "    - A circulant shift value (0, 1, 2, 3) becomes a shifted identity matrix.\n",
        "    \"\"\"\n",
        "    rows, cols = base_matrix.shape\n",
        "    H_expanded = torch.zeros((rows * Z, cols * Z), dtype=torch.float32)\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            shift = base_matrix[i, j].item()\n",
        "            if shift == -1:\n",
        "                # Zero Block (ZxZ Zero Matrix)\n",
        "                H_expanded[i * Z:(i + 1) * Z, j * Z:(j + 1) * Z] = torch.zeros((Z, Z))\n",
        "            else:\n",
        "                # Circulant Shifted Identity Matrix\n",
        "                I_Z = torch.eye(Z)  # Identity Matrix of size ZxZ\n",
        "                I_Z = torch.roll(I_Z, shifts=shift, dims=1)  # Shift columns by \"shift\" value\n",
        "                H_expanded[i * Z:(i + 1) * Z, j * Z:(j + 1) * Z] = I_Z\n",
        "\n",
        "    return H_expanded\n",
        "\n",
        "# Example: Expanding BM1 with Z=64\n",
        "Z_selected = 64  # Example Z value\n",
        "H_expanded_BM1 = expand_base_matrix(BM1, Z_selected)  # Full Parity-Check Matrix for BM1\n",
        "H_expanded_BM2 = expand_base_matrix(BM2, Z_selected)  # Full Parity-Check Matrix for BM2\n",
        "\n",
        "# Display results\n",
        "df_BM1 = pd.DataFrame(BM1.numpy())\n",
        "df_BM2 = pd.DataFrame(BM2.numpy())\n",
        "\n",
        "print(df_BM1)\n",
        "print(df_BM2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K4VlpirsWkr"
      },
      "source": [
        "# 1.2 Short-length 5G LDPC code 🔴 (finish by 21 Feb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lafu_SQ6P5mP"
      },
      "source": [
        "#Check Layer ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qNk6piQPIbu",
        "outputId": "5e2ba407-1821-4e55-ec11-a05c4eb65f68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reordered Selected Values:\n",
            " tensor([[ 0.8000,  0.0000],\n",
            "        [ 0.1000,  0.0000],\n",
            "        [ 0.2000,  0.0000],\n",
            "        [ 0.2000,  0.3000],\n",
            "        [-0.1000,  0.3000],\n",
            "        [-0.1000,  0.2000],\n",
            "        [-0.5000,  0.0000],\n",
            "        [-0.9000,  0.0000],\n",
            "        [ 0.1000,  0.0000],\n",
            "        [-0.3000,  0.0000],\n",
            "        [ 0.3000,  0.2000],\n",
            "        [ 0.2000,  0.2000],\n",
            "        [ 0.2000,  0.3000],\n",
            "        [ 0.7000,  0.0000]])\n",
            "\n",
            "Min-Sum Result:\n",
            " tensor([[ 0.0000,  0.0000,  0.0000,  0.2000, -0.1000, -0.1000, -0.0000],\n",
            "        [-0.0000,  0.0000, -0.0000,  0.2000,  0.2000,  0.2000,  0.0000]])\n"
          ]
        }
      ],
      "source": [
        "def min_sum_operation_reordered(input_tensor, index_tensor):\n",
        "    \"\"\"\n",
        "    For each row in input_tensor, apply every row in index_tensor, collect results,\n",
        "    and apply the Min-Sum operation along columns, with batch results stacked together.\n",
        "\n",
        "    Handles -1 indices by replacing them with `num_vars` and extending input_tensor with extra zeros.\n",
        "\n",
        "    Args:\n",
        "        input_tensor (torch.Tensor): Shape (batch_size, num_vars)\n",
        "        index_tensor (torch.Tensor): Shape (num_index_rows, num_selected_indices)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Min-Sum results after applying index_tensor to input_tensor.\n",
        "                      Shape: (batch_size * num_index_rows, num_selected_indices)\n",
        "    \"\"\"\n",
        "    batch_size, num_vars = input_tensor.shape  # (B, V)\n",
        "    num_index_rows, num_selected_indices = index_tensor.shape  # (I, S)\n",
        "\n",
        "    # Step 1: Create a mask for valid indices (ignoring -1)\n",
        "    valid_mask = index_tensor != -1\n",
        "\n",
        "    # Step 2: Replace -1 with a safe dummy index (`num_vars`)\n",
        "    safe_indices = index_tensor.clone()\n",
        "    safe_indices[~valid_mask] = num_vars  # Safe index (out of range)\n",
        "\n",
        "    # Step 3: Extend input_tensor with an extra column of zeros\n",
        "    input_extended = torch.cat([input_tensor, torch.zeros((batch_size, 1), dtype=input_tensor.dtype)], dim=1)\n",
        "    # Shape: (batch_size, num_vars + 1)\n",
        "\n",
        "    # Step 4: Expand input tensor to apply all index_tensor rows\n",
        "    input_expanded = input_extended.unsqueeze(0).expand(num_index_rows, -1, -1)\n",
        "    # Shape: (num_index_rows, batch_size, num_vars + 1)\n",
        "\n",
        "    # Step 5: Expand index tensor to match batch dimension\n",
        "    index_expanded = safe_indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
        "    # Shape: (num_index_rows, batch_size, num_selected_indices)\n",
        "\n",
        "    # Step 6: Gather elements\n",
        "    selected_values = torch.gather(input_expanded, dim=2, index=index_expanded)\n",
        "    # Shape: (num_index_rows, batch_size, num_selected_indices)\n",
        "\n",
        "    # Step 7: Apply mask: Set invalid gathered values (from -1) to zero\n",
        "    selected_values[~valid_mask.unsqueeze(1).expand(-1, batch_size, -1)] = 0\n",
        "\n",
        "    # Step 8: Rearrange `selected_values` to have all batch 1 results first\n",
        "    selected_values_reordered = selected_values.permute(1, 0, 2).reshape(batch_size * num_index_rows, num_selected_indices)\n",
        "    # Shape: (batch_size * num_index_rows, num_selected_indices)\n",
        "\n",
        "    # Step 9: Apply Min-Sum\n",
        "    sign_product = torch.prod(torch.sign(selected_values_reordered), dim=1)  # Sign product\n",
        "    min_abs = torch.min(torch.abs(selected_values_reordered), dim=1).values  # Min absolute value\n",
        "\n",
        "    # Step 10: Compute Min-Sum result\n",
        "    min_sum_result = sign_product * min_abs  # Element-wise multiplication\n",
        "    # Shape: (batch_size * num_index_rows,)\n",
        "\n",
        "    return selected_values_reordered, min_sum_result.reshape(batch_size, num_index_rows)\n",
        "\n",
        "# Example input message tensor (batch_size=2, num_vars=7)\n",
        "input_tensor = torch.tensor([\n",
        "    [0.2, -0.5, 0.8, -0.1, 0.2, 0.3, 0.1],  # Batch 1\n",
        "    [-0.3, 0.7, -0.9, 0.2, 0.3, 0.2, 0.1]   # Batch 2\n",
        "], dtype=torch.float32)  # Shape: (2, 7)\n",
        "\n",
        "# Check LLR Index Matrix:\n",
        "#  tensor([[ 2, -1],\n",
        "#         [ 6, -1],\n",
        "#         [ 0, -1],\n",
        "#         [ 4,  5],\n",
        "#         [ 3,  5],\n",
        "#         [ 3,  4],\n",
        "#         [ 1, -1]])\n",
        "\n",
        "# Compute Min-Sum with reordered results\n",
        "selected_values_reordered, check_layer_output = min_sum_operation_reordered(input_tensor, check_LLR_matrix)\n",
        "\n",
        "print(\"Reordered Selected Values:\\n\", selected_values_reordered)\n",
        "print(\"\\nMin-Sum Result:\\n\", check_layer_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN3OvB94u_9Q"
      },
      "source": [
        "# Variable Layer ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pTm7oYobPVi",
        "outputId": "4cec1794-5375-4c85-aef7-e7e13857ba37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Variable Messages:\n",
            " tensor([[ 0.0000,  0.0000,  0.2000,  0.0000,  0.0000,  0.0000, -0.1000],\n",
            "        [-0.3000, -0.3000,  0.9000,  0.7000, -0.9000,  0.2000,  0.4000]])\n"
          ]
        }
      ],
      "source": [
        "def variable_layer_update(input_mapping_LLR, check_to_variable_messages, variable_index_tensor):\n",
        "    \"\"\"\n",
        "    Performs the Variable Node Update in LDPC Decoding using Min-Sum.\n",
        "\n",
        "    Args:\n",
        "        LLR (torch.Tensor): Initial log-likelihood ratios (batch_size, num_vars).\n",
        "        check_to_variable_messages (torch.Tensor): Messages from check nodes (batch_size, num_messages).\n",
        "        variable_index_tensor (torch.Tensor): Indices of check node messages that share the same column.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Updated variable node messages (batch_size, num_vars).\n",
        "    \"\"\"\n",
        "    batch_size, num_vars = check_to_variable_messages.shape  # (B, V)\n",
        "    num_vars_mapped, max_neighbors = variable_index_tensor.shape  # (V', S)\n",
        "    num_messages = check_to_variable_messages.shape[1]  # Number of check node messages\n",
        "\n",
        "    # Create a mask where valid indices are True (ignoring -1 values)\n",
        "    valid_mask = variable_index_tensor != -1\n",
        "\n",
        "    # Replace -1 with a safe dummy index (num_messages) which we will zero out later\n",
        "    safe_indices = variable_index_tensor.clone()\n",
        "    safe_indices[~valid_mask] = num_messages  # Use an out-of-range index***\n",
        "\n",
        "    # Extend check_to_variable_messages with a row of zeros at index num_messages\n",
        "    extended_check_to_variable = torch.cat([\n",
        "        check_to_variable_messages,\n",
        "        torch.zeros((batch_size, 1), dtype=check_to_variable_messages.dtype)\n",
        "    ], dim=1)  # Shape: (batch_size, num_messages + 1)\n",
        "\n",
        "    # Expand check-to-variable messages and indices\n",
        "    check_to_variable_expanded = extended_check_to_variable.unsqueeze(0).expand(num_vars_mapped, -1, -1)\n",
        "    index_expanded = safe_indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
        "\n",
        "    # Gather messages using safe indices\n",
        "    gathered_messages = torch.gather(check_to_variable_expanded, dim=2, index=index_expanded)\n",
        "\n",
        "    # Apply mask: Set invalid gathered values to 0\n",
        "    gathered_messages[~valid_mask.unsqueeze(1).expand(-1, batch_size, -1)] = 0\n",
        "\n",
        "    # Sum valid messages\n",
        "    summed_messages = torch.sum(gathered_messages, dim=2)  # (num_vars_mapped, batch_size)\n",
        "\n",
        "    # Transpose to match (batch_size, num_vars)\n",
        "    summed_messages = summed_messages.T  # (batch_size, num_vars_mapped)\n",
        "\n",
        "    # Compute updated variable messages\n",
        "    variable_messages = input_mapping_LLR + summed_messages  # Add the LLR to the sum of messages\n",
        "\n",
        "    return variable_messages\n",
        "\n",
        "# Example Check-to-Variable Messages (batch_size=2, num_messages=3)\n",
        "var_input_tensor = check_layer_output\n",
        "\n",
        "# Compute Variable Node Messages\n",
        "variable_messages = variable_layer_update(input_mapping_LLR, var_input_tensor, var_LLR_matrix)\n",
        "\n",
        "print(\"Updated Variable Messages:\\n\", variable_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwCmKzKvscMD"
      },
      "source": [
        "# Residual Connections ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X9mlsxVpFbq"
      },
      "outputs": [],
      "source": [
        "# params\n",
        "batch_size = 2\n",
        "num_nodes = nodes\n",
        "depth_L = 2\n",
        "\n",
        "class LDPCDecoderResidual(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Neural LDPC decoder with residual connections.\n",
        "    Implements Equation (4) from the paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes, depth_L=2):\n",
        "        super(LDPCDecoderResidual, self).__init__()\n",
        "\n",
        "        self.num_nodes = num_nodes  # Number of variable nodes\n",
        "        self.depth_L = depth_L  # Depth of residual connections\n",
        "\n",
        "        # Trainable weights\n",
        "        self.w_ch = torch.nn.Parameter(torch.ones(num_variables))  # w^{ch} weights\n",
        "        self.w_res = torch.nn.Parameter(torch.ones(depth_L))  # w^{(l-t)} weights for residual connections\n",
        "\n",
        "    def forward(self, LLR, check_messages):\n",
        "        \"\"\"\n",
        "        Compute variable node update with residual connections.\n",
        "\n",
        "        Args:\n",
        "            LLR (torch.Tensor): Initial LLR values (batch_size, num_variables)\n",
        "            check_messages (torch.Tensor): Incoming check node messages (batch_size, num_variables)\n",
        "\n",
        "        Returns:\n",
        "            Q (torch.Tensor): Updated variable node messages (batch_size, num_variables, num_iterations)\n",
        "        \"\"\"\n",
        "        batch_size = LLR.shape[0]\n",
        "\n",
        "        # Initialize message tensor\n",
        "        Q = torch.zeros(batch_size, self.num_nodes, self.num_iterations, device=LLR.device)\n",
        "\n",
        "        # for l in range(self.num_iterations):\n",
        "        # Compute weighted channel LLR contribution\n",
        "        weighted_LLR = self.w_ch * LLR\n",
        "\n",
        "        # Compute check node contribution\n",
        "        check_contrib = check_messages  # Shape: (batch_size, num_variables)\n",
        "\n",
        "        # Residual connection contribution from previous iterations\n",
        "        res_contrib = torch.zeros_like(LLR)\n",
        "        for t in range(1, min(self.depth_L + 1, l + 1)):  # Ensure only past iterations are used\n",
        "            res_contrib += self.w_res[t - 1] * Q[:, :, l - t]\n",
        "\n",
        "        # Final update equation (Equation 4)\n",
        "        Q[:, :, l] = weighted_LLR + check_contrib + res_contrib\n",
        "\n",
        "        return Q  # Shape: (batch_size, num_variables, num_iterations)\n",
        "\n",
        "\n",
        "\n",
        "# num_variables = H.shape  # Example: 10 variable nodes\n",
        "# num_check_nodes = 5  # Example: 5 check nodes\n",
        "\n",
        "# residual_input or something that stores the previous L Variable layers\n",
        "\n",
        "# Initialize decoder\n",
        "decoder = LDPCDecoderResidual(num_nodes, depth_L)\n",
        "\n",
        "# Run decoder\n",
        "Q_output = decoder(residual_input, check_layer_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS0jhDVwosfk"
      },
      "source": [
        "# Output Layer ✅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59HmhZLqowcA",
        "outputId": "358d6420-826d-4170-da93-69717dd8adbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Entropy Loss:\n",
            " tensor([[0.6931, 0.6931, 0.6931, 0.5981, 0.6444, 0.6444, 0.6931],\n",
            "        [0.6931, 0.6931, 0.6931, 0.5981, 0.7981, 0.5981, 0.6931]])\n",
            "\n",
            "Max Cross-Entropy Loss Per Batch:\n",
            " tensor([0.6931, 0.7981])\n",
            "\n",
            "Final LLRs:\n",
            " tensor([[ 0.0000,  0.0000,  0.0000,  0.2000, -0.1000, -0.1000, -0.0000],\n",
            "        [-0.0000,  0.0000, -0.0000,  0.2000,  0.2000,  0.2000,  0.0000]])\n",
            "\n",
            "Soft-Bit Values (After applying Sigmoid to LLR):\n",
            " tensor([[0.5000, 0.5000, 0.5000, 0.5498, 0.4750, 0.4750, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5498, 0.5498, 0.5498, 0.5000]])\n",
            "\n",
            "Hard-Bit Values:\n",
            " tensor([[0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 1., 1., 0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LDPCDecoderOutputLayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Output mapping layer for LDPC neural decoder.\n",
        "    Maps the final LLRs to soft-bit values using Sigmoid and applies Cross-Entropy loss.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LDPCDecoderOutputLayer, self).__init__()\n",
        "\n",
        "    def forward(self, final_LLR, ground_truth, input_mapping_LLR):\n",
        "        \"\"\"\n",
        "        Computes the soft-bit values and cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            final_LLR (torch.Tensor): Output LLR vector from the last variable layer. Shape: (batch_size, num_variables).\n",
        "            ground_truth (torch.Tensor): True transmitted bits (0 or 1). Shape: (batch_size, num_variables).\n",
        "\n",
        "        Returns:\n",
        "            loss (torch.Tensor): Cross-entropy loss.\n",
        "            predicted_bits (torch.Tensor): Decoded soft-bit values.\n",
        "        \"\"\"\n",
        "\n",
        "        result = final_LLR + input_mapping_LLR\n",
        "\n",
        "        # Step 1: Apply Sigmoid to final LLR (σ(L_o))\n",
        "        soft_bits = torch.sigmoid(final_LLR)  # Maps LLRs to probability values (0 to 1)\n",
        "\n",
        "        # Step 2: Compute Cross-Entropy loss\n",
        "        loss = F.binary_cross_entropy(soft_bits, ground_truth, reduction='none')  # Cross-entropy per bit\n",
        "        print(\"Cross-Entropy Loss:\\n\", loss)\n",
        "\n",
        "        # Step 3: Apply max function over the loss vector (for FER minimization)\n",
        "        max_loss = torch.max(loss, dim=1).values  # Maximum cross-entropy value per batch\n",
        "\n",
        "        return max_loss, soft_bits\n",
        "\n",
        "# # Example Usage\n",
        "batch_size = 2\n",
        "num_variables = 7  # Number of message nodes\n",
        "\n",
        "# # Set the random seed for reproducibility\n",
        "torch.manual_seed(23)\n",
        "\n",
        "# Simulated LLR output from the final variable layer\n",
        "final_LLR = check_layer_output\n",
        "\n",
        "# Simulated ground truth bits (binary values: 0 or 1)\n",
        "ground_truth = torch.randint(0, 2, (batch_size, num_variables)).float()\n",
        "\n",
        "# Initialize and run output mapping\n",
        "output_layer = LDPCDecoderOutputLayer()\n",
        "max_loss, predicted_bits = output_layer(final_LLR, ground_truth, input_mapping_LLR)\n",
        "\n",
        "hard_bit_results = (predicted_bits > 0.5).float()\n",
        "\n",
        "# Print results\n",
        "print(\"\\nMax Cross-Entropy Loss Per Batch:\\n\", max_loss)\n",
        "print(\"\\nFinal LLRs:\\n\", final_LLR)\n",
        "print(\"\\nSoft-Bit Values (After applying Sigmoid to LLR):\\n\", predicted_bits)\n",
        "print(\"\\nHard-Bit Values:\\n\", hard_bit_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLPXvZN6jUej"
      },
      "source": [
        "# To-Dos\n",
        "\n",
        "1.   Output Mapping : what operation is used on the output mapping layer\n",
        "2.   Test the whole network with 1 iteration, which inlcudes 1x CL and VL\n",
        "3.   Check to see if we need to do anything to use cyclic property of H matrix\n",
        "4.   Check if there is a threshold for calculating FER? i.e If errors per frame > x, then count as one whole frame is defective\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCR_tZmVbd4c"
      },
      "source": [
        "# Obtaining LLR"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QxhqhnUb2nXW",
        "igyqVDWfsPBX",
        "xXcNSo6usnlI",
        "q1dv7lxX3Io6",
        "EgTJu4mSr_eY",
        "kGjESNlHvFpX",
        "Lafu_SQ6P5mP",
        "mN3OvB94u_9Q",
        "lwCmKzKvscMD"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
